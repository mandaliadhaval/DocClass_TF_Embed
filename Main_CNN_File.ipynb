{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "EMBEDDING_SIZE = 20\n",
    "N_FILTERS = 10\n",
    "WINDOW_SIZE = 20\n",
    "FILTER_SHAPE1 = [WINDOW_SIZE, EMBEDDING_SIZE]\n",
    "FILTER_SHAPE2 = [WINDOW_SIZE, N_FILTERS]\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "n_words = 0\n",
    "MAX_LABEL = 15\n",
    "WORDS_FEATURE = 'words'  # Name of the input words feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(features, labels, mode):\n",
    "  \"\"\"2 layer ConvNet to predict from sequence of words to a class.\"\"\"\n",
    "  # Convert indexes of words into embeddings.\n",
    "  # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "  # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "  # EMBEDDING_SIZE].\n",
    "  word_vectors = tf.contrib.layers.embed_sequence(\n",
    "      features[WORDS_FEATURE], vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "  word_vectors = tf.expand_dims(word_vectors, 3)\n",
    "  with tf.variable_scope('CNN_Layer1'):\n",
    "    # Apply Convolution filtering on input sequence.\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        word_vectors,\n",
    "        filters=N_FILTERS,\n",
    "        kernel_size=FILTER_SHAPE1,\n",
    "        padding='VALID',\n",
    "        # Add a ReLU for non linearity.\n",
    "        activation=tf.nn.relu)\n",
    "    # Max pooling across output of Convolution+Relu.\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        conv1,\n",
    "        pool_size=POOLING_WINDOW,\n",
    "        strides=POOLING_STRIDE,\n",
    "        padding='SAME')\n",
    "    # Transpose matrix so that n_filters from convolution becomes width.\n",
    "    pool1 = tf.transpose(pool1, [0, 1, 3, 2])\n",
    "  with tf.variable_scope('CNN_Layer2'):\n",
    "    # Second level of convolution filtering.\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        pool1,\n",
    "        filters=N_FILTERS,\n",
    "        kernel_size=FILTER_SHAPE2,\n",
    "        padding='VALID')\n",
    "    # Max across each filter to get useful features for classification.\n",
    "    pool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1])\n",
    "\n",
    "  # Apply regular WX + B and classification.\n",
    "  logits = tf.layers.dense(pool2, MAX_LABEL, activation=None)\n",
    "\n",
    "  predicted_classes = tf.argmax(logits, 1)\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={\n",
    "            'class': predicted_classes,\n",
    "            'prob': tf.nn.softmax(logits)\n",
    "        })\n",
    "\n",
    "  onehot_labels = tf.one_hot(labels, MAX_LABEL, 1, 0)\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  eval_metric_ops = {\n",
    "      'accuracy': tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predicted_classes)\n",
    "  }\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "  global n_words\n",
    "  # Prepare training and testing data\n",
    "  dbpedia = tf.contrib.learn.datasets.load_dataset(\n",
    "      'dbpedia', test_with_fake_data=FLAGS.test_with_fake_data)\n",
    "  x_train = pandas.DataFrame(dbpedia.train.data)[1]\n",
    "  y_train = pandas.Series(dbpedia.train.target)\n",
    "  x_test = pandas.DataFrame(dbpedia.test.data)[1]\n",
    "  y_test = pandas.Series(dbpedia.test.target)\n",
    "\n",
    "  # Process vocabulary\n",
    "  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "      MAX_DOCUMENT_LENGTH)\n",
    "  x_train = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "  x_test = np.array(list(vocab_processor.transform(x_test)))\n",
    "  n_words = len(vocab_processor.vocabulary_)\n",
    "  print('Total words: %d' % n_words)\n",
    "\n",
    "  # Build model\n",
    "  classifier = tf.estimator.Estimator(model_fn=cnn_model)\n",
    "\n",
    "  # Train.\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={WORDS_FEATURE: x_train},\n",
    "      y=y_train,\n",
    "      batch_size=len(x_train),\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "  classifier.train(input_fn=train_input_fn, steps=100)\n",
    "\n",
    "  # Predict.\n",
    "  test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={WORDS_FEATURE: x_test},\n",
    "      y=y_test,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "  predictions = classifier.predict(input_fn=test_input_fn)\n",
    "  y_predicted = np.array(list(p['class'] for p in predictions))\n",
    "  y_predicted = y_predicted.reshape(np.array(y_test).shape)\n",
    "\n",
    "  # Score with sklearn.\n",
    "  score = metrics.accuracy_score(y_test, y_predicted)\n",
    "  print('Accuracy (sklearn): {0:f}'.format(score))\n",
    "\n",
    "  # Score with tensorflow.\n",
    "  scores = classifier.evaluate(input_fn=test_input_fn)\n",
    "  print('Accuracy (tensorflow): {0:f}'.format(scores['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded dbpedia_csv.tar.gz 68431223 bytes.\n",
      "Total words: 7552\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\dhaval\\AppData\\Local\\Temp\\tmp5c0udpxc\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': 1, '_model_dir': 'C:\\\\Users\\\\dhaval\\\\AppData\\\\Local\\\\Temp\\\\tmp5c0udpxc', '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_session_config': None, '_save_checkpoints_secs': 600}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\dhaval\\AppData\\Local\\Temp\\tmp5c0udpxc\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.70809, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into C:\\Users\\dhaval\\AppData\\Local\\Temp\\tmp5c0udpxc\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.000314293.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\dhaval\\AppData\\Local\\Temp\\tmp5c0udpxc\\model.ckpt-100\n",
      "Accuracy (sklearn): 0.428571\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-08-15:00:16\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\dhaval\\AppData\\Local\\Temp\\tmp5c0udpxc\\model.ckpt-100\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-08-15:00:17\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.428571, global_step = 100, loss = 6.95133\n",
      "Accuracy (tensorflow): 0.428571\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhaval\\Anaconda3\\envs\\tensorflowpy3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--test_with_fake_data',\n",
    "      default=False,\n",
    "      help='Test the example code with fake data.',\n",
    "      action='store_true')\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
