{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self):\n",
    "        self.embedding_size = 100\n",
    "        self.batch_size = 8\n",
    "        self.num_skips = 2\n",
    "        self.skip_window = 1\n",
    "        self.num_epochs = 30\n",
    "        self.learning_rate = 0.1\n",
    "\n",
    "        self.current_index = 0\n",
    "        self.words = []\n",
    "\n",
    "        self.dictionary = {}\n",
    "        self.final_embeddings = None\n",
    "\n",
    "    def build_dataset(self):\n",
    "        new_word_id = 0\n",
    "        self.words = []\n",
    "        self.dictionary = {}\n",
    "\n",
    "        \n",
    "        for filename in glob.glob(\"./corpus/*.txt\"):\n",
    "            with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "                text = f.read()\n",
    "                text = text.lower().replace(\"\\n\", \" \")\n",
    "                text = re.sub(r\"[^a-z '\\-]\", \"\", text)\n",
    "                text = re.sub(r\"[ ]+\", \" \", text)\n",
    "\n",
    "                for word in text.split():\n",
    "        \n",
    "                    if word.startswith(\"-\"): continue \n",
    "                    if word not in self.dictionary:\n",
    "                        self.dictionary[word] = new_word_id\n",
    "                        new_word_id += 1\n",
    "                    self.words.append(self.dictionary[word])\n",
    "\n",
    "        \n",
    "        self.vocabulary_size = new_word_id\n",
    "        print(\"# of distinct words:\", new_word_id)\n",
    "        print(\"# of total words:\", len(self.words))\n",
    "\n",
    "    \n",
    "    def generate_batch(self):\n",
    "        \n",
    "        assert self.batch_size % self.num_skips == 0\n",
    "        assert self.num_skips <= 2 * self.skip_window\n",
    "        \n",
    "        self.current_index = 0\n",
    "        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(self.batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        \n",
    "        span = 2 * self.skip_window + 1\n",
    "        if self.current_index + span >= len(self.words):\n",
    "            raise StopIteration\n",
    "\n",
    "        \n",
    "        buffer = collections.deque(maxlen=span)\n",
    "        for _ in range(span):\n",
    "            buffer.append(self.words[self.current_index])\n",
    "            self.current_index += 1\n",
    "\n",
    "        \n",
    "        for _ in range(len(self.words) // self.batch_size):\n",
    "            \n",
    "            for i in range(self.batch_size // self.num_skips):\n",
    "                target = self.skip_window\n",
    "                targets_to_avoid = [self.skip_window]\n",
    "                \n",
    "                for j in range(self.num_skips):\n",
    "                    while target in targets_to_avoid:\n",
    "                        target = random.randint(0, span - 1)\n",
    "                    targets_to_avoid.append(target)\n",
    "                    batch[i * self.num_skips + j] = buffer[self.skip_window]\n",
    "                    labels[i * self.num_skips + j, 0] = buffer[target]\n",
    "\n",
    "                \n",
    "                buffer.append(self.words[self.current_index])\n",
    "                self.current_index += 1\n",
    "                if self.current_index >= len(self.words):\n",
    "                    raise StopIteration\n",
    "            yield batch, labels\n",
    "        raise StopIteration\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
    "        print(\"Start train\")\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([self.vocabulary_size, self.embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(self.embedding_size)))\n",
    "        \n",
    "        nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "        \n",
    "        \n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "        \n",
    "                \n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        print(\"Features:\")\n",
    "        print(\"nce_weights:\", nce_weights,\n",
    "              \"nce_biases:\",nce_biases,\n",
    "              \"embed:\", embed, \n",
    "              \"train_labels:\", train_labels, \n",
    "              \"self.batch_size // 2: \", self.batch_size // 2,\n",
    "              \"self.batch_size:\", self.batch_size, \n",
    "              \"self.vocabulary_size: \", self.vocabulary_size)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed, self.batch_size // 2, self.vocabulary_size)\n",
    "        )\n",
    "        \n",
    "        print(\"Loss:\", loss)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        \n",
    "        # For similarities\n",
    "        valid_examples = np.random.choice(100, 16, replace=False)\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        \n",
    "        print(\"Session: Start\")\n",
    "        logdir = \"./corpus/log\"\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        \n",
    "            for epoch in range(self.num_epochs):\n",
    "                epoch_loss = 0\n",
    "                \n",
    "                #generate_batch()\n",
    "                print(\"Generating Batch: Started\")\n",
    "                \n",
    "                for batch_x, batch_y in self.generate_batch():\n",
    "                    _, loss_value = sess.run([optimizer, loss], feed_dict={train_inputs: batch_x, train_labels: batch_y})\n",
    "                    epoch_loss += loss_value\n",
    "                print(\"Generating Batch: End\")\n",
    "                print(\"Epoch\", epoch, \"completed out of\", self.num_epochs, \"-- loss:\", epoch_loss)\n",
    "                \n",
    "                print(\"Saving Saver Session\", logdir)\n",
    "                # Embeddings Visualization\n",
    "                saver.save(sess, logdir + \"/blog.ckpt\", epoch)\n",
    "\n",
    "            \n",
    "            self.final_embeddings = normalized_embeddings.eval() # <class 'numpy.ndarray'>\n",
    "\n",
    "            # Embeddings Visualiation\n",
    "            summary_writer = tf.summary.FileWriter(logdir)\n",
    "            config = projector.ProjectorConfig()\n",
    "            embedding = config.embeddings.add()\n",
    "            embedding.tensor_name = embeddings.name \n",
    "            embedding.metadata_path = \"./corpus/model/blog.metadata.tsv\"\n",
    "            projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "            self.plot()\n",
    "\n",
    "        print(\"Dumping Dictionary\")\n",
    "        with open(\"./corpus/model/blog.dic\", \"wb\") as f:\n",
    "            pickle.dump(self.dictionary, f)\n",
    "        print(\"Dictionary was saved to\", \"./corpus/model/blog.dic\")\n",
    "        np.save(\"./corpus/model/blog.npy\", self.final_embeddings)\n",
    "        print(\"Embeddings were saved to\", \"./corpus/model/blog.npy/\")\n",
    "\n",
    "        # Embeddings Visualization\n",
    "        \n",
    "        sorted_dict = sorted(self.dictionary.items(), key=lambda x: x[1])\n",
    "        words = [\"{}\\n\".format(x[0]) for x in sorted_dict]\n",
    "        with open(\"./corpus/model/blog.metadata.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(words)\n",
    "        print(\"Embeddings metadata was saved to ./corpus/model/blog.metadata.tsv\")\n",
    "\n",
    "    def plot(self, filename=\"./corpus/model/blog.png\"):\n",
    "        tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=5000)\n",
    "        plot_only=500\n",
    "        low_dim_embeddings = tsne.fit_transform(self.final_embeddings[:plot_only, :])\n",
    "        reversed_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
    "        labels = [reversed_dictionary[i] for i in range(plot_only)]\n",
    "\n",
    "        plt.figure(figsize=(18, 18))\n",
    "        for i, label in enumerate(labels):\n",
    "            x, y = low_dim_embeddings[i, :]\n",
    "            plt.scatter(x, y)\n",
    "            plt.annotate(label,\n",
    "                        xy=(x, y),\n",
    "                        xytext=(5, 2),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha=\"right\",\n",
    "                        va=\"bottom\")\n",
    "        plt.savefig(filename)\n",
    "        print(\"Scatter plot was saved to\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of distinct words: 253854\n",
      "# of total words: 17005207\n"
     ]
    }
   ],
   "source": [
    "corpus.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train\n",
      "Features:\n",
      "nce_weights: <tf.Variable 'Variable_40:0' shape=(253854, 100) dtype=float32_ref> nce_biases: <tf.Variable 'Variable_41:0' shape=(253854,) dtype=float32_ref> embed: Tensor(\"embedding_lookup_16:0\", shape=(8, 100), dtype=float32) train_labels: Tensor(\"Placeholder_27:0\", shape=(8, 1), dtype=int32) self.batch_size // 2:  4 self.batch_size: 8 self.vocabulary_size:  253854\n",
      "Loss: Tensor(\"Mean_5:0\", shape=(), dtype=float32)\n",
      "Session: Start\n",
      "Generating Batch: Started\n",
      "Generating Batch: End\n",
      "Epoch 0 completed out of 30 -- loss: 13135643.502\n",
      "Saving Saver Session ./corpus/log\n",
      "Generating Batch: Started\n",
      "Generating Batch: End\n",
      "Epoch 1 completed out of 30 -- loss: 5344041.84242\n",
      "Saving Saver Session ./corpus/log\n",
      "Generating Batch: Started\n",
      "Generating Batch: End\n",
      "Epoch 2 completed out of 30 -- loss: 4230715.36069\n",
      "Saving Saver Session ./corpus/log\n",
      "Generating Batch: Started\n",
      "Generating Batch: End\n",
      "Epoch 3 completed out of 30 -- loss: 3771777.57246\n",
      "Saving Saver Session ./corpus/log\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to WriteFile: ./corpus/log/blog.ckpt-3.data-00000-of-00001.tempstate9185156088408496772 : There is not enough space on the disk.\r\n; Unknown error\n\t [[Node: save_3/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_3/Const_0_0, save_3/SaveV2/tensor_names, save_3/SaveV2/shape_and_slices, Variable, Variable_1, Variable_10, Variable_11, Variable_12, Variable_13, Variable_14, Variable_15, Variable_16, Variable_17, Variable_18, Variable_19, Variable_2, Variable_20, Variable_21, Variable_22, Variable_23, Variable_24, Variable_25, Variable_26, Variable_27, Variable_28, Variable_29, Variable_3, Variable_30, Variable_31, Variable_32, Variable_33, Variable_34, Variable_35, Variable_36, Variable_37, Variable_38, Variable_39, Variable_4, Variable_40, Variable_41, Variable_5, Variable_6, Variable_7, Variable_8, Variable_9)]]\n\nCaused by op 'save_3/SaveV2', defined at:\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-59-3682e42dd59a>\", line 1, in <module>\n    corpus.train()\n  File \"<ipython-input-56-e47c15c2d00d>\", line 133, in train\n    saver = tf.train.Saver()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1139, in __init__\n    self.build()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 689, in build\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 276, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 219, in save_op\n    tensors)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 745, in save_v2\n    tensors=tensors, name=name)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nUnknownError (see above for traceback): Failed to WriteFile: ./corpus/log/blog.ckpt-3.data-00000-of-00001.tempstate9185156088408496772 : There is not enough space on the disk.\r\n; Unknown error\n\t [[Node: save_3/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_3/Const_0_0, save_3/SaveV2/tensor_names, save_3/SaveV2/shape_and_slices, Variable, Variable_1, Variable_10, Variable_11, Variable_12, Variable_13, Variable_14, Variable_15, Variable_16, Variable_17, Variable_18, Variable_19, Variable_2, Variable_20, Variable_21, Variable_22, Variable_23, Variable_24, Variable_25, Variable_26, Variable_27, Variable_28, Variable_29, Variable_3, Variable_30, Variable_31, Variable_32, Variable_33, Variable_34, Variable_35, Variable_36, Variable_37, Variable_38, Variable_39, Variable_4, Variable_40, Variable_41, Variable_5, Variable_6, Variable_7, Variable_8, Variable_9)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to WriteFile: ./corpus/log/blog.ckpt-3.data-00000-of-00001.tempstate9185156088408496772 : There is not enough space on the disk.\r\n; Unknown error\n\t [[Node: save_3/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_3/Const_0_0, save_3/SaveV2/tensor_names, save_3/SaveV2/shape_and_slices, Variable, Variable_1, Variable_10, Variable_11, Variable_12, Variable_13, Variable_14, Variable_15, Variable_16, Variable_17, Variable_18, Variable_19, Variable_2, Variable_20, Variable_21, Variable_22, Variable_23, Variable_24, Variable_25, Variable_26, Variable_27, Variable_28, Variable_29, Variable_3, Variable_30, Variable_31, Variable_32, Variable_33, Variable_34, Variable_35, Variable_36, Variable_37, Variable_38, Variable_39, Variable_4, Variable_40, Variable_41, Variable_5, Variable_6, Variable_7, Variable_8, Variable_9)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-3682e42dd59a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-e47c15c2d00d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saving Saver Session\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;31m# Embeddings Visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m                 \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/blog.ckpt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         model_checkpoint_path = sess.run(\n\u001b[0;32m   1471\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m             {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to WriteFile: ./corpus/log/blog.ckpt-3.data-00000-of-00001.tempstate9185156088408496772 : There is not enough space on the disk.\r\n; Unknown error\n\t [[Node: save_3/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_3/Const_0_0, save_3/SaveV2/tensor_names, save_3/SaveV2/shape_and_slices, Variable, Variable_1, Variable_10, Variable_11, Variable_12, Variable_13, Variable_14, Variable_15, Variable_16, Variable_17, Variable_18, Variable_19, Variable_2, Variable_20, Variable_21, Variable_22, Variable_23, Variable_24, Variable_25, Variable_26, Variable_27, Variable_28, Variable_29, Variable_3, Variable_30, Variable_31, Variable_32, Variable_33, Variable_34, Variable_35, Variable_36, Variable_37, Variable_38, Variable_39, Variable_4, Variable_40, Variable_41, Variable_5, Variable_6, Variable_7, Variable_8, Variable_9)]]\n\nCaused by op 'save_3/SaveV2', defined at:\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-59-3682e42dd59a>\", line 1, in <module>\n    corpus.train()\n  File \"<ipython-input-56-e47c15c2d00d>\", line 133, in train\n    saver = tf.train.Saver()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1139, in __init__\n    self.build()\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 689, in build\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 276, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 219, in save_op\n    tensors)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 745, in save_v2\n    tensors=tensors, name=name)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\dhavalma\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nUnknownError (see above for traceback): Failed to WriteFile: ./corpus/log/blog.ckpt-3.data-00000-of-00001.tempstate9185156088408496772 : There is not enough space on the disk.\r\n; Unknown error\n\t [[Node: save_3/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_3/Const_0_0, save_3/SaveV2/tensor_names, save_3/SaveV2/shape_and_slices, Variable, Variable_1, Variable_10, Variable_11, Variable_12, Variable_13, Variable_14, Variable_15, Variable_16, Variable_17, Variable_18, Variable_19, Variable_2, Variable_20, Variable_21, Variable_22, Variable_23, Variable_24, Variable_25, Variable_26, Variable_27, Variable_28, Variable_29, Variable_3, Variable_30, Variable_31, Variable_32, Variable_33, Variable_34, Variable_35, Variable_36, Variable_37, Variable_38, Variable_39, Variable_4, Variable_40, Variable_41, Variable_5, Variable_6, Variable_7, Variable_8, Variable_9)]]\n"
     ]
    }
   ],
   "source": [
    "corpus.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dhavalma\\\\AnacondaProjects\\\\Document Classification'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
