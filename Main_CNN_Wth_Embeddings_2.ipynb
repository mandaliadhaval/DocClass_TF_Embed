{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self):\n",
    "        self.embedding_size = 100\n",
    "        self.batch_size = 8\n",
    "        self.num_skips = 2\n",
    "        self.skip_window = 1\n",
    "        self.num_epochs = 30\n",
    "        self.learning_rate = 0.1\n",
    "\n",
    "        self.current_index = 0\n",
    "        self.words = []\n",
    "\n",
    "        self.dictionary = {}\n",
    "        self.final_embeddings = None\n",
    "\n",
    "    def build_dataset(self):\n",
    "        new_word_id = 0\n",
    "        self.words = []\n",
    "        self.dictionary = {}\n",
    "\n",
    "        \n",
    "        for filename in glob.glob(\"./corpus/*.txt\"):\n",
    "            with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "                text = f.read()\n",
    "                text = text.lower().replace(\"\\n\", \" \")\n",
    "                text = re.sub(r\"[^a-z '\\-]\", \"\", text)\n",
    "                text = re.sub(r\"[ ]+\", \" \", text)\n",
    "\n",
    "                for word in text.split():\n",
    "        \n",
    "                    if word.startswith(\"-\"): continue \n",
    "                    if word not in self.dictionary:\n",
    "                        self.dictionary[word] = new_word_id\n",
    "                        new_word_id += 1\n",
    "                    self.words.append(self.dictionary[word])\n",
    "\n",
    "        \n",
    "        self.vocabulary_size = new_word_id\n",
    "        print(\"# of distinct words:\", new_word_id)\n",
    "        print(\"# of total words:\", len(self.words))\n",
    "\n",
    "    \n",
    "    def generate_batch(self):\n",
    "        \n",
    "        assert self.batch_size % self.num_skips == 0\n",
    "        assert self.num_skips <= 2 * self.skip_window\n",
    "        \n",
    "        self.current_index = 0\n",
    "        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(self.batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        \n",
    "        span = 2 * self.skip_window + 1\n",
    "        if self.current_index + span >= len(self.words):\n",
    "            raise StopIteration\n",
    "\n",
    "        \n",
    "        buffer = collections.deque(maxlen=span)\n",
    "        for _ in range(span):\n",
    "            buffer.append(self.words[self.current_index])\n",
    "            self.current_index += 1\n",
    "\n",
    "        \n",
    "        for _ in range(len(self.words) // self.batch_size):\n",
    "            \n",
    "            for i in range(self.batch_size // self.num_skips):\n",
    "                target = self.skip_window\n",
    "                targets_to_avoid = [self.skip_window]\n",
    "                \n",
    "                for j in range(self.num_skips):\n",
    "                    while target in targets_to_avoid:\n",
    "                        target = random.randint(0, span - 1)\n",
    "                    targets_to_avoid.append(target)\n",
    "                    batch[i * self.num_skips + j] = buffer[self.skip_window]\n",
    "                    labels[i * self.num_skips + j, 0] = buffer[target]\n",
    "\n",
    "                \n",
    "                buffer.append(self.words[self.current_index])\n",
    "                self.current_index += 1\n",
    "                if self.current_index >= len(self.words):\n",
    "                    raise StopIteration\n",
    "            yield batch, labels\n",
    "        raise StopIteration\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
    "        print(\"Start train\")\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([self.vocabulary_size, self.embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(self.embedding_size)))\n",
    "        print(\"1\")\n",
    "        nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "        print(\"2\")\n",
    "        \n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "\n",
    "        \n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels, self.batch_size // 2, self.vocabulary_size)\n",
    "        )\n",
    "\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        \n",
    "        # For similarities\n",
    "        # valid_examples = np.random.choice(100, 16, replace=False)\n",
    "        # valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = int(embeddings / norm)\n",
    "        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "        print(valid_embeddings, normalized_embeddings)\n",
    "        similarity = tf.matmul(int(valid_embeddings), int(normalized_embeddings), transpose_b=True)\n",
    "\n",
    "        logdir = \"./corpus/log\"\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "            for epoch in range(self.num_epochs):\n",
    "                epoch_loss = 0\n",
    "                # generate_batch()\n",
    "                for batch_x, batch_y in self.generate_batch():\n",
    "                    _, loss_value = sess.run([optimizer, loss], feed_dict={train_inputs: batch_x, train_labels: batch_y})\n",
    "                    epoch_loss += loss_value\n",
    "\n",
    "                print(\"Epoch\", epoch, \"completed out of\", self.num_epochs, \"-- loss:\", epoch_loss)\n",
    "\n",
    "                # Embeddings Visualization\n",
    "                saver.save(sess, logdir + \"/blog.ckpt\", epoch)\n",
    "\n",
    "            \n",
    "            self.final_embeddings = normalized_embeddings.eval() # <class 'numpy.ndarray'>\n",
    "\n",
    "            # Embeddings Visualiation\n",
    "            summary_writer = tf.summary.FileWriter(logdir)\n",
    "            config = projector.ProjectorConfig()\n",
    "            embedding = config.embeddings.add()\n",
    "            embedding.tensor_name = embeddings.name \n",
    "            embedding.metadata_path = \"./corpus/model/blog.metadata.tsv\"\n",
    "            projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "        # self.plot()\n",
    "\n",
    "        \n",
    "        with open(\"./corpus/model/blog.dic\", \"wb\") as f:\n",
    "            pickle.dump(self.dictionary, f)\n",
    "        print(\"Dictionary was saved to\", \"./corpus/model/blog.dic\")\n",
    "        np.save(\"./corpus/model/blog.npy\", self.final_embeddings)\n",
    "        print(\"Embeddings were saved to\", \"./corpus/model/blog.npy/\")\n",
    "\n",
    "        # Embeddings Visualization\n",
    "        \n",
    "        sorted_dict = sorted(self.dictionary.items(), key=lambda x: x[1])\n",
    "        words = [\"{}\\n\".format(x[0]) for x in sorted_dict]\n",
    "        with open(\"./corpus/model/blog.metadata.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(words)\n",
    "        print(\"Embeddings metadata was saved to ./corpus/model/blog.metadata.tsv\")\n",
    "\n",
    "    def plot(self, filename=\"./corpus/model/blog.png\"):\n",
    "        tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=5000)\n",
    "        plot_only=500\n",
    "        low_dim_embeddings = tsne.fit_transform(self.final_embeddings[:plot_only, :])\n",
    "        reversed_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
    "        labels = [reversed_dictionary[i] for i in range(plot_only)]\n",
    "\n",
    "        plt.figure(figsize=(18, 18))\n",
    "        for i, label in enumerate(labels):\n",
    "            x, y = low_dim_embeddings[i, :]\n",
    "            plt.scatter(x, y)\n",
    "            plt.annotate(label,\n",
    "                        xy=(x, y),\n",
    "                        xytext=(5, 2),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha=\"right\",\n",
    "                        va=\"bottom\")\n",
    "        plt.savefig(filename)\n",
    "        print(\"Scatter plot was saved to\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of distinct words: 253854\n",
      "# of total words: 17005207\n"
     ]
    }
   ],
   "source": [
    "corpus.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.generate_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Corpus' object has no attribute 'vocabulary_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3682e42dd59a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-c65a7de37665>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         embeddings = tf.Variable(\n\u001b[1;32m---> 90\u001b[1;33m             tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Corpus' object has no attribute 'vocabulary_size'"
     ]
    }
   ],
   "source": [
    "corpus.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
